# Alertmanager configuration for ARA AI

global:
  resolve_timeout: 5m
  # SMTP configuration for email alerts
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@ara-ai.com'
  smtp_auth_username: 'your-email@gmail.com'
  smtp_auth_password: 'your-app-password'
  smtp_require_tls: true

# Templates for alert notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree for alert routing
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m
    
    # Warning alerts - less frequent
    - match:
        severity: warning
      receiver: 'warning-alerts'
      repeat_interval: 1h
    
    # Info alerts - daily digest
    - match:
        severity: info
      receiver: 'info-alerts'
      repeat_interval: 24h

# Alert receivers
receivers:
  - name: 'default'
    email_configs:
      - to: 'team@ara-ai.com'
        headers:
          Subject: '[ARA AI] Alert: {{ .GroupLabels.alertname }}'
        html: |
          <h2>Alert: {{ .GroupLabels.alertname }}</h2>
          <p><strong>Severity:</strong> {{ .CommonLabels.severity }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>

  - name: 'critical-alerts'
    email_configs:
      - to: 'oncall@ara-ai.com'
        headers:
          Subject: '[CRITICAL] ARA AI Alert: {{ .GroupLabels.alertname }}'
        html: |
          <h1 style="color: red;">CRITICAL ALERT</h1>
          <h2>{{ .GroupLabels.alertname }}</h2>
          <p><strong>Component:</strong> {{ .CommonLabels.component }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>
          <p><strong>Action Required:</strong> Immediate investigation needed</p>
    
    # Slack webhook (optional)
    # slack_configs:
    #   - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
    #     channel: '#ara-ai-alerts'
    #     title: 'CRITICAL: {{ .GroupLabels.alertname }}'
    #     text: '{{ .CommonAnnotations.summary }}'
    
    # PagerDuty (optional)
    # pagerduty_configs:
    #   - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
    #     description: '{{ .CommonAnnotations.summary }}'

  - name: 'warning-alerts'
    email_configs:
      - to: 'team@ara-ai.com'
        headers:
          Subject: '[WARNING] ARA AI: {{ .GroupLabels.alertname }}'
        html: |
          <h2 style="color: orange;">Warning Alert</h2>
          <h3>{{ .GroupLabels.alertname }}</h3>
          <p><strong>Component:</strong> {{ .CommonLabels.component }}</p>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>
          <p><strong>Time:</strong> {{ .StartsAt }}</p>

  - name: 'info-alerts'
    email_configs:
      - to: 'monitoring@ara-ai.com'
        headers:
          Subject: '[INFO] ARA AI Daily Digest'
        html: |
          <h2>Information Alert</h2>
          <h3>{{ .GroupLabels.alertname }}</h3>
          <p><strong>Summary:</strong> {{ .CommonAnnotations.summary }}</p>
          <p><strong>Description:</strong> {{ .CommonAnnotations.description }}</p>

# Inhibition rules - suppress certain alerts when others are firing
inhibit_rules:
  # Suppress warning alerts if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'component']
  
  # Suppress info alerts if warning or critical is firing
  - source_match_re:
      severity: 'critical|warning'
    target_match:
      severity: 'info'
    equal: ['alertname', 'component']
  
  # Suppress downstream alerts if API is down
  - source_match:
      alertname: 'APIDown'
    target_match_re:
      alertname: '.*'
    equal: ['cluster']
